{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      " 43%|████▎     | 12/28 [10:06<13:38, 51.16s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load SST-2 dataset and specify the validation split\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "\n",
    "# List of models you want to use (with new models included)\n",
    "models_info = [\n",
    "    {\"name\": \"flan-t5\", \"model_name\": \"google/flan-t5-base\", \"model_type\": \"seq2seq\"},\n",
    "    {\"name\": \"bloom\", \"model_name\": \"bigscience/bloom-560m\", \"model_type\": \"causal\"},\n",
    "    # {\"name\": \"opt\", \"model_name\": \"facebook/opt-1.3b\", \"model_type\": \"causal\"},\n",
    "    # {\"name\": \"llama\", \"model_name\": \"meta-llama/Llama-2-7b-hf\", \"model_type\": \"causal\"},\n",
    "    {\"name\": \"bart\", \"model_name\": \"facebook/bart-base\", \"model_type\": \"seq2seq\"},\n",
    "    # {\"name\": \"blenderbot\", \"model_name\": \"facebook/blenderbot-400M-distill\", \"model_type\": \"seq2seq\"},\n",
    "    {\"name\": \"distilbert\", \"model_name\": \"distilbert-base-uncased\", \"model_type\": \"embedding\"},\n",
    "    {\"name\": \"minilm\", \"model_name\": \"microsoft/MiniLM-L12-H384-uncased\", \"model_type\": \"embedding\"},\n",
    "    {\"name\": \"electra\", \"model_name\": \"google/electra-small-discriminator\", \"model_type\": \"embedding\"}\n",
    "]\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = []\n",
    "for info in models_info:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(info[\"model_name\"])\n",
    "    if info[\"model_type\"] in [\"seq2seq\", \"causal\"]:\n",
    "        if info[\"model_type\"] == \"seq2seq\":\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(info[\"model_name\"]).to('cuda' if torch.cuda.is_available() else 'cpu', non_blocking=True)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(info[\"model_name\"]).to('cuda' if torch.cuda.is_available() else 'cpu', non_blocking=True)\n",
    "    else:  # Embedding models (DistilBERT, MiniLM, Electra)\n",
    "        model = AutoModel.from_pretrained(info[\"model_name\"]).to('cuda' if torch.cuda.is_available() else 'cpu', non_blocking=True)\n",
    "    \n",
    "    models.append((tokenizer, model, info[\"name\"], info[\"model_type\"]))\n",
    "\n",
    "# Batch size for processing\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Function to generate output from a model in batch\n",
    "def generate_output_batch(input_texts, tokenizer, model, model_type):\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    \n",
    "    if model_type == \"seq2seq\":\n",
    "        # Set max_new_tokens instead of max_length to control output size without truncating input\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
    "        output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        return output_texts\n",
    "    \n",
    "    elif model_type == \"causal\":\n",
    "        # Set max_new_tokens for causal models as well\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "        output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        return output_texts\n",
    "    \n",
    "    else:  # Embedding-based models\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Use CLS token or mean pooling for embedding-based models\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embedding for classification\n",
    "        return embeddings  # Return the embedding vector for classification tasks\n",
    "\n",
    "# Store output data\n",
    "output_data = []\n",
    "\n",
    "# Process data in batches\n",
    "for i in tqdm(range(0, len(dataset), BATCH_SIZE)):\n",
    "    batch = dataset.select(range(i, min(i+BATCH_SIZE, len(dataset))))\n",
    "    input_texts = [example[\"sentence\"] for example in batch]\n",
    "\n",
    "    # For each model, generate output in batch and store result\n",
    "    for tokenizer, model, model_name, model_type in models:\n",
    "        output_texts = generate_output_batch(input_texts, tokenizer, model, model_type)\n",
    "        \n",
    "        for input_text, output_text in zip(input_texts, output_texts):\n",
    "            # If it's an embedding, convert to string for storage\n",
    "            if model_type == \"embedding\":\n",
    "                output_text = str(output_text.tolist())  # Store embedding as string\n",
    "        \n",
    "            output_data.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"generated_text\": output_text,\n",
    "                \"model_name\": model_name\n",
    "            })\n",
    "\n",
    "# Create DataFrame from the output data\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_df.to_csv(\"valid_df.csv\", index=False)\n",
    "\n",
    "print(\"New dataset saved as 'valid_df.csv'.\")\n",
    "print(output_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
